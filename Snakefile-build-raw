"""
Snakefile for doing the first stages of data processing from the daq sandbox files
to the blinded raw data. It handles:
- moving the daq files from the sandbox to the sorted file system
- running build raw on this data (with trimming)
- blinding the physics data
"""

import pathlib, os, json, sys
from scripts.util.patterns import (
    get_pattern_unsorted_data,
    get_pattern_tier_daq,
    get_pattern_tier_raw,
)
from scripts.util.utils import (
    subst_vars_in_snakemake_config,
    runcmd,
    config_path,
    chan_map_path,
    filelist_path,
    pars_path,
    metadata_path,
)
from scripts.util.pars_loading import pars_catalog
import scripts.util as ds

check_in_cycle = True

# Set with `snakemake --configfile=/path/to/your/config.json`
# configfile: "have/to/specify/path/to/your/config.json"

subst_vars_in_snakemake_config(workflow, config)

setup = config["setups"]["l200"]
configs = config_path(setup)
chan_maps = chan_map_path(setup)
swenv = runcmd(setup)
meta = metadata_path(setup)

basedir = workflow.basedir


wildcard_constraints:
    experiment=r"\w+",
    period=r"p\d{2}",
    run=r"r\d{3}",
    datatype=r"\w{3}",
    timestamp=r"\d{8}T\d{6}Z",


localrules:
    gen_filelist,
    autogen_output,


onstart:
    print("INFO: starting workflow")

    ds.pars_key_resolve.write_par_catalog(
        ["-*-*-*-cal"],
        os.path.join(pars_path(setup), "raw", "validity.jsonl"),
        [
            get_pattern_unsorted_data(setup),
            get_pattern_tier_daq(setup),
            get_pattern_tier_raw(setup),
        ],
        {"cal": ["par_raw"]},
    )


include: "rules/common.smk"
include: "rules/main.smk"
include: "rules/raw.smk"
include: "rules/blinding_check.smk"


rule gen_filelist:
    """Generate file list.

    It is a checkpoint so when it is run it will update the dag passed on the
    files it finds as an output. It does this by taking in the search pattern,
    using this to find all the files that match this pattern, deriving the keys
    from the files found and generating the list of new files needed.
    """
    input:
        lambda wildcards: get_filelist(
            wildcards,
            setup,
            get_pattern(wildcards.tier),
            ignore_keys_file=os.path.join(configs, "empty_keys.keylist"),
            analysis_runs_file=None,
        ),
    output:
        os.path.join(filelist_path(setup), "{label}-{tier}.filelist"),
    run:
        with open(output[0], "w") as f:
            for fn in input:
                f.write(f"{fn}\n")


rule sort_data:
    """
    This rules moves the daq data from the unsorted sandbox dir
    to the sorted dirs under generated
    """
    input:
        get_pattern_unsorted_data(setup),
    output:
        get_pattern_tier_daq(setup),
    shell:
        "mv {input} {output}"
