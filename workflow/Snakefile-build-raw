"""
Snakefile for doing the first stages of data processing from the daq sandbox files
to the blinded raw data. It handles:
- moving the daq files from the sandbox to the sorted file system
- running build raw on this data (with trimming)
- blinding the physics data
"""

import os, sys
from pathlib import Path
from legenddataflow import patterns as patt
from legenddataflow import utils, execenv, ParsKeyResolve
import legenddataflow.paths as paths
from datetime import datetime
from dbetto import AttrsDict, TextDB
from legendmeta import LegendMetadata
from legenddataflow.pre_compile_catalog import pre_compile_catalog
from legenddataflow.build_filelist import get_filelist

os.environ["XDG_CACHE_HOME"] = config.get("XDG_CACHE_HOME", ".snakemake/cache")

utils.subst_vars_in_snakemake_config(workflow, config)
config = AttrsDict(config)

check_in_cycle = True
swenv = execenv.execenv_prefix(config)
meta_path = paths.metadata_path(config)
det_status = paths.det_status_path(config)
configs = paths.config_path(config)
chan_maps = paths.chan_map_path(config)
meta = paths.metadata_path(config)

channelmap_textdb = pre_compile_catalog(Path(chan_maps) / "channelmaps")

time = datetime.now().strftime("%Y%m%dT%H%M%SZ")

# NOTE: this will attempt a clone of legend-metadata, if the directory does not exist
metadata = LegendMetadata(meta_path, lazy=True)
if "legend_metadata_version" in config:
    metadata.checkout(config.legend_metadata_version)


wildcard_constraints:
    experiment=r"\w+",
    period=r"p\d{2}",
    run=r"r\d{3}",
    datatype=r"\w{3}",
    timestamp=r"\d{8}T\d{6}Z",


localrules:
    gen_filelist,
    autogen_output,


include: "rules/common.smk"
include: "rules/main.smk"
include: "rules/raw.smk"
include: "rules/blinding_check.smk"


onstart:
    print("INFO: initializing workflow")

    # Make sure some packages are initialized before we send jobs to avoid race conditions
    if not workflow.touch:
        shell(
            execenv.execenv_pyexe(config, "python") + " -c 'import daq2lh5, matplotlib'"
        )

    raw_par_cat_file = Path(paths.pars_path(config)) / "raw" / "validity.yaml"
    if raw_par_cat_file.is_file():
        raw_par_cat_file.unlink()
    try:
        Path(raw_par_cat_file).parent.mkdir(parents=True, exist_ok=True)
        raw_par_catalog.write_to(raw_par_cat_file)
    except NameError:
        print("WARNING: no raw parameter catalog found")


onsuccess:
    shell("rm -f *.gen")
    shell(f"rm -rf {paths.filelist_path(config)}/*")


rule gen_filelist:
    input:
        lambda wildcards: get_filelist(
            wildcards,
            config,
            get_search_pattern(config, wildcards.tier),
            ignore_keys_file=Path(det_status) / "ignored_daq_cycles.yaml",
            analysis_runs_file=Path(det_status) / "runlists.yaml",
        ),
    output:
        temp(Path(paths.filelist_path(config)) / "{label}-{tier}.filelist"),
    script:
        "src/legenddataflow/scripts/write_filelist.py"


rule sort_data:
    """
    Move DAQ data from sandbox to organized folder.

    This rules moves the DAQ data from the unsorted sandbox directory to the
    correct location in the `tier_raw` folder.
    """
    input:
        patt.get_pattern_tier_daq_unsorted(config, extension="{ext}"),
    output:
        patt.get_pattern_tier_daq(config, extension="{ext}"),
    shell:
        "mv {input} {output}"


rule compress_data:
    """
    Compress raw data. It uses pbzip2 to compress the data in parallel.
    Currently only for ORCA data.
    """
    input:
        patt.get_pattern_tier_daq(config, extension="orca"),
    output:
        patt.get_pattern_tier_daq(config, extension="orca.bz2"),
    threads: 10
    shell:
        "pbzip2 -p{threads} -z {input}"
